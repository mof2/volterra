'''
preg is a Python module for doing Gaussian process regression [1] using a polynomial
covariance function. It can be used to find a Volterra or Wiener series expansion of an
unknown system where only pairs of vector-valued inputs and scalar outputs are given [2].

The package provides the following functionalities:

* Automatic model selection according to one of three possible criteria:

    a. the log-likelihood of the observed data [1];
    b. Geisser's surrogate predictive probability [3];
    c. the analytically computed leave-one-out error on the training set [4].

* Computation of the predictive distribution (i.e., predictive mean and variance) of the
  output for a new, previously unseen input.

* Computation of the explicit nth-order Volterra operator from the implicitly given
  polynomial expansion (see [2])

The available polynomial covariance functions of order p are

    1. the inhomogeneous polynomial kernel:  k(x, y) = (x*y + 1)^p
    2. the summed polynomial kernel: k(x,y) = sum_i^p (x*y)^i
    3. the adaptive polynomial kernel: k(x,y) = sum_i^p w_i (x*y)^i where each degree of
       nonlinearity receives an individual weight w_i that is found during model
       selection.

Example of use::

    # init Gaussian process object
    gp = Preg(logger, covariance, degree, hyperparameters)

    # do automatic model selection for polynomial degree 1 to 4 and training
    gp.amsd(Xtrain, ytrain, [1,2,3,4], model_selection_method, number_of_iterations)

    # predict on test data
    predicted_test_outputs = gp.predict(test_inputs)
    
    # estimate third order Volterra kernel (i.e. a third order tensor of coefficients)
    volterra_kernel_3 = gp.volt(3)

A simple 1D toy example showing the basic regression functionality is given in the
accompanying programming example 'sinc_example.py'. Further examples can be found in the
test file 'test_preg.py'. preg can be used together with scikit-learn, i.e. it implements
the scikit-lean-API-functions set_params(), get_params(), fit(), and predict() for being
included in the cross-validation estimator and pipelines. Console output is fed to a
logger based on the standard python module logging. The logger has to provided by the user
at startup.

To install, unpack the python distribution, then execute::

    python setup.py install

The module is available as::

    import preg

The tests can be run by typing in the installation directory (nose must be installed)::

    nosetests -v

This documentation can be generated by typing in the subdirectory doc::

    make html

The automatic model selection uses a minimization routine based on Carl Rasmussen's
minimize.m MATLAB script (see copyright notice in the function code) and its Python
adaptation by R. Memisevic (2008).

References:

[1] Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian processes for machine
learning. Cambridge, MA: MIT Press.

[2] Franz, M. O. and B. Schoelkopf (2006). A unifying view of Wiener and Volterra theory
and polynomial kernel regression. Neural Computation 18(12), 3097-3118.

[3] S. Sundararajan, S.S. Keerthi (2001). Predictive approaches for choosing
hyperparameters in Gaussian processes. Neural Computation 13, 1103-1118.

[4] V. Vapnik (1982). Estimation of dependences based on empirical data. New York:
Springer.
'''
import logging
from math import exp, log, pi, floor, factorial
import numpy as np
import scipy.linalg as LA

# read in version number from preg_version module
from preg_version import __version__

class Preg:
    '''
    Gaussian process. Contains all necessary parameters for doing Gaussian process
    regression and intermediary results that can be used to speed up processing. A GP
    object is instantiated as follows::

        gp = Preg(logger, covariance, degree, hyperparameters)

    The first parameter is a logging object that must be an instance of logging.Logger
    from the standard logging module. All console output is fed to this logger.

    Currently, the following covariances are available:

    * 'ihp': inhomogenous polynomial kernel
    * 'sp': summed polynomial kernel
    * 'ap': adaptive polynomial kernel

    The degree must be > 1 if you use a polynomial kernel.

    The hyperparameters must be a one-dimensional array of size >= 2. The first two
    parameters have a fixed meaning:

    * hyp[0] is the natural logarithm of the signal standard deviation.
    * hyp[1] is the natural logarithm of the noise standard deviation.
    * hyp[2]-hyp[degree + 2] are the natural logarithms of the weight factors w_i of the
      adaptive polynomial kernel (ap).

    The reason why the log of the parameters are used is that this often leads to a better
    conditioned (and unconstrained) optimization problem than using the raw
    hyperparameters themselves.
    The training outputs must be a one-dimensional array with the same number n of entries
    as in the training inputs.
    '''
    def __init__(self, logger=[], covtype='ihp', degree=1, hyp=[]):

        # set parameters without sanity check (done later in ams, amsd, fit, predict)
        self.logger = logger
        self.covtype = covtype
        self.degree = degree
        self.hyp = hyp

        # internal variables
        self.X_ = []                  # training inputs
        self.mu_ = []                 # training target mean
        self.G_ = []                  # Gram matrix
        self.K_ = []                  # Covariance matrix
        self.D_ = []                  # Distance matrix (only needed for Gaussian kernel)
        self.xy_ = []                 # inner product matrix
        self.invK_ = []               # Inverse covariance matrix
        self.invKt_ = []              # invK * target
        self.logdetK_ = []            # log of determinant

        # startup message
        if isinstance(self.logger, logging.Logger):
            self.logger.info('Preg Ver. {}, IOS Konstanz'.format(__version__))


    # return a printable representation of the GP object
    def __repr__(self):
        '''
        returns a printable representation of the GP object such that it is possible to
        create this object.
        '''
        return 'Preg(logger={}, covtype={}, degree={}, hyp={})'.format(self.logger,
            self.covtype, self.degree, self.hyp)


    # set parameters of GP object (no parameter check is done)
    def set_params(self, **parameters):
        '''
        Standard method for setting the GP object parameters::

            gp.set_params(logger=lg, covtype=ct, degree=deg, hyp=h)

        This method is needed by the cross-validation module of scikit-learn for cloning
        the GP learner. In accordance with their coding guideline, the method does not
        check the parameters for validity. This is only done when the GP is used in amd(),
        amsd(), etc.
        '''
        for parameter, value in parameters.items():
            setattr(self, parameter, value)


    # return parameters
    def get_params(self, deep=True):
        '''
        Standard method for getting GP parameters::

            param_dict = gp.get_params()

        returns all GP parameters as a dictionary. This method is needed by the
        cross-validation module of scikit-learn for cloning the GP learner. The parameter
        'deep' is provided for scikit-learn compatibility, but ignored.
        '''
        return {
            "logger": self.logger,
            "covtype": self.covtype,
            "degree": self.degree,
            "hyp": self.hyp}


    # check parameters
    def check_params(self):
        '''
        Internal function for checking the validity of all GP parameters. If no valid
        logger is provided, i.e. no instance of logging.Logger, an internal mute logger is
        allocated. Degree and hyperparameters are converted to float and numpy.float64,
        respectively.
        '''
        # check logger
        if not isinstance(self.logger, logging.Logger):
            self.logger = logging.getLogger('preg')
            self.logger.setLevel(logging.ERROR)

        # check covariance
        if self.covtype not in ['ihp', 'sp', 'ap']:
            self.logger.warn('preg: unkown covariance: {}'.format(self.covtype))
            return False

        # check degree
        if self.degree < 1:
            self.logger.warn('preg: degree of polynomial kernel must be > 0.')
            return False
        if self.degree != floor(self.degree):
            self.logger.warn('preg: degree of polynomial kernel must an integer number.')
            return False

        # check hyperparameters
        if type(self.hyp) != np.ndarray:
            self.hyp = np.asarray(self.hyp)
        if self.hyp.dtype != 'float64':
            self.hyp = self.hyp.astype('float64')
        if self.hyp.ndim != 1:
            self.logger.warn('preg: initial hyperparameters must be a numpy vector.')
            return False
        if self.covtype in ['sp', 'ihp'] and np.shape(self.hyp)[0] != 2:
            self.logger.warn('preg: hyperparameter for sp or ihp kernel must be a 2-vector.')
            return False
        if self.covtype == 'ap' and np.shape(self.hyp)[0] != 2 + self.degree:
            self.logger.warn('preg: hyperparameter for ap must be a vector of length 2 or degree + 2.')
            return False

        return True


    # check input data
    def check_data(self, X, y):
        '''
        Internal function for checking the validity and size compatibility of the provided
        data.
        '''
        # check inputs
        if type(X) != np.ndarray:
            self.logger.warn('preg: input data is not a *NumPy* array')
            return False
        if X.ndim > 2:
            self.logger.warn('preg: training input must be given as vector or matrix.')
            return False

        # check targets
        if type(y) != np.ndarray:
            self.logger.warn('preg: target data is not a *NumPy* array')
            return False
        if y.ndim != 1:
            self.logger.warn('preg: targets must be a vector.')
            return False
        if np.shape(y)[0] != np.shape(X)[0]:
            self.logger.warn('preg: number of targets must match number of inputs.')
            return False

        return True


    # train GP
    def fit(self, X, y):
        '''
        trains a Gaussian Process predictor for given training inputs and targets::

            gp.fit(X_train, y_train)

        The training inputs 'X_train' and outputs 'y_train' are used to internally compute
        a covariance matrix based on the set of hyperparameters. Usually, these parameters
        are set before by calling ams() or amsd(). The covariance matrix is inverted and
        the regression weights computed and stored internally.

        The method returns the object (self). This pattern is useful to be able to
        implement quick one liners in an IPython session such as::

            y_pred = Preg(covtype='ihp', degree=3, hyp=[1.2, 0.3]).fit(X, y).predict(Xt)

        The training inputs have to be provided as rows of a numpy matrix. So, if you have
        n D-dimensional vectors, X_train must be an n x D double matrix, and y_train a
        n-vector.
        '''
        # self check
        if not self.check_params():
            return []

        # check training data
        if not self.check_data(X, y):
            return []
        if X.dtype != 'float64':
            X = X.astype('float64')
        if y.dtype != 'float64':
            y = y.astype('float64')

        # fit GP to training data
        self.mu_ = y.mean()
        self.X_ = X.copy()
        self.gram(self.hyp)
        self.cov(self.hyp)
        self.invert_cov('gpp')
        self.invKt_ = np.dot(self.invK_, y - self.mu_)

        return self


    # predict mean
    def predict(self, X, y=0):
        '''
        predicts the mean of the GP for new, previously unseen test inputs::

            mu = predict(test)

        returns the predicted means for the test input as a numpy vector. The test input
        must be a m x D vector array where the dimensionality of the vectors is the same
        as that of the training data vectors. An automatic model selection
        has to be run or a fitting to the training data before to get the optimal
        hyperparameters. In addition, most of the computationally expensive terms are
        precomputed during ams() or amsd() which greatly accelerates the prediction. The
        addition parameter 'y' is only included for scikit-learn pipeline compatibility
        and ignored.
        '''
        # self check
        if not self.check_params():
            return []
        if len(self.K_) == 0:
            self.logger.warn('preg: no training data saved, cannot predict.')
            return []

        # check test input
        if type(X) != np.ndarray:
            self.logger.warn('preg: test input is not a *NumPy* array')
            return []
        if X.ndim > 2:
            self.logger.warn('preg: test input must be 1- or 2-dimensional')
            return []
        if ((X.ndim > 1 and self.X_.ndim > 1 and np.shape(X)[1] !=
            np.shape(self.X_)[1]) or X.ndim != self.X_.ndim):
            self.logger.warn('preg: dimension of training and test input is different.')
            return []
        if X.dtype != 'float64':
            X = X.astype('float64')

        # cross-covariance
        vs = self.hyp[0]        # signal variance
        Qt = exp(2*vs)*self.kernel(self.X_, X, self.hyp)

        # predicted means
        return np.dot(Qt.T, self.invKt_) + self.mu_


    # automatic model selection
    def ams(self, X, y, modsel='gpp', n_iter=10):
        '''
        automatic model selection::

            optval = ams(X_ms, y_ms, model_selection, n_iter)

        does an automatic model selection by gradient descent in hyperparameter space.
        The input and output data 'X_ms' and 'y_ms' do not need to be the same as the
        final training data. Usually a smaller set is used to save time.

        As objective functions for finding the hyperparameters in the model selection step
        we have:

        * 'llh': log-likelihood of the training data
        * 'gpp': Geissers surrogate predictive probability
        * 'loo': Leave-one-out error

        When 'loo' is chosen as evaluation criterion, the
        derivative w.r.t. the signal standard deviation is always set to 0 (although it is
        not 0 in reality) such that it remains constant during minimization. This ensures
        better convergence since the 'loo' criterion is invariant under scaling of the
        hyperparameters. The resulting signal and noise standard deviations reflect only
        the proper signal-to-noise ratio, not the correct absolute values. This means that
        the predicted variances in pred_meanvar() can only be determined up to a scale
        factor.

        The initial value for the hyperparameter is set during initialisation of the GP,
        afterwards the initial values are replaced by the optimal parameter set. In
        contrast to amsd(), this optimization leaves the degree of the polynomial
        untouched, so optimization is only performed for the the hyperparameters. The
        argument n_iter gives the maximum number of line searches before the optimization
        is stopped. The method returns (self).
        '''
        # self check
        if not self.check_params():
            return []

        # check model selection data
        if not self.check_data(X, y):
            return []
        if X.dtype != 'float64':
            X = X.astype('float64')
        if y.dtype != 'float64':
            y = y.astype('float64')

        # check other parameters
        if modsel not in ['llh', 'gpp', 'loo']:
            self.logger.warn('preg: unkown model selection method: {}'.format(modsel))
            return []
        if type(n_iter) != int:
            n_iter = int(n_iter)
        if n_iter < 1:
            self.logger.warn('preg: number of iterations must be > 0.')
            return []

        self.mu_ = y.mean()
        self.X_ = X.copy()
        self.gram(self.hyp)
        opthyp, its_needed, optval = self.minimize(y, self.hyp, modsel, n_iter)
        self.hyp = opthyp.copy()
        self.logger.info('Used {:d} iterations, minval {:.04g}'.format(its_needed, optval))
        return self


    # automatic model selection with choice of optimal polynomial degree
    def amsd(self, X, y, degs, modsel='gpp', n_iter=10):
        '''
        automatic model selection  with choice of optimal polynomial degree::

            optval = amsd(X_ms, y_ms, degs, model_selection, n_iter)

        does an automatic model selection by gradient descent in hyperparameter space.
        The input and output data 'X_ms' and 'y_ms' do not need to be the same as the
        final training data. Usually a smaller set is used to save time.

        As objective functions for finding the hyperparameters in the model selection step
        we have:

        * 'llh': log-likelihood of the training data
        * 'gpp': Geissers surrogate predictive probability
        * 'loo': Leave-one-out error

        When 'loo' is chosen as evaluation criterion, the
        derivative w.r.t. the signal standard deviation is always set to 0 (although it is
        not 0 in reality) such that it remains constant during minimization. This ensures
        better convergence since the 'loo' criterion is invariant under scaling of the
        hyperparameters. The resulting signal and noise standard deviations reflect only
        the proper signal-to-noise ratio, not the correct absolute values. This means that
        the predicted variances in pred_meanvar() can only be determined up to a scale
        factor.

        The initial value for the hyperparameter is set during initialisation of the GP,
        afterwards the initial values are replaced by the optimal parameter set. In
        contrast to ams(), this optimization chooses also the optimal degree of the
        polynomial kernel. As such, it can be only used with a polynomial kernel. The
        argument nit gives the maximum number of line searches before the optimization is
        stopped. degs is a list of polynomial degree that are to be checked.
        The method returns (self).
        '''
        # self check
        if not self.check_params():
            return []

        # check model selection data
        if not self.check_data(X, y):
            return []
        if X.dtype != 'float64':
            X = X.astype('float64')
        if y.dtype != 'float64':
            y = y.astype('float64')

        # check degrees
        if type(degs) != list:
            degs = list(degs)
        for deg in degs:
            if type(deg) != float:
                deg = float(deg)
            if deg < 1.0:
                self.logger.warn('preg: degree of polynomial kernel must be > 0.')
                return []
            if deg != floor(deg):
                self.logger.warn('preg: degree of polynomial kernel must an integer number.')
                return []

        # check other parameters
        if modsel not in ['llh', 'gpp', 'loo']:
            self.logger.warn('preg: unkown model selection method: {}'.format(modsel))
            return []
        if type(n_iter) != int:
            n_iter = int(n_iter)
        if n_iter < 1:
            self.logger.warn('preg: number of iterations must be > 0.')
            return []

        # init
        self.mu_ = y.mean()
        self.X_ = X.copy()
        min_e = np.inf
        hyp0 = self.hyp.copy() # save initial values

        # try out all polynomial degrees
        for deg in degs:

            # init hyperparameters to same value for each degree
            if self.covtype == 'ap':
                self.hyp = np.zeros(deg + 2)
                hyplen = np.shape(hyp0)[0]
                for i in np.arange(0, deg + 2):
                    if i < hyplen:
                        self.hyp[i] = hyp0[i]
                    else:
                        self.hyp[i] = -(i - 2)
            else:
                self.hyp = hyp0.copy()
            self.degree = deg

            # Gram matrix (needed only once during minimization for ihp, sp)
            if self.covtype in ['ihp', 'sp']:
                self.gram(self.hyp)

            # find best hyperparams
            opthyp, its_needed, optval = self.minimize(y, self.hyp, modsel, n_iter)
            self.logger.debug(
                'Degree: {:d}, used {:d} line searches, minval {:.04g}'.format(deg,
                    its_needed, optval))

            # retain information if the current performance is best
            if optval < min_e:
                min_e = optval
                optdeg = deg
                hyp = opthyp.copy()
                optG = self.G_.copy()
                optK = self.K_.copy()
                opt_invK = self.invK_.copy()
                opt_invKt = self.invKt_.copy()

        # update GP object
        self.degree = optdeg
        self.hyp = hyp.copy()
        self.K_ = optK.copy()
        self.G_ = optG.copy()
        self.invK_ = opt_invK.copy()
        self.invKt_ = opt_invKt.copy()
        return self


    # Gram matrix
    def gram(self, hyp):
        '''
        computes the Gram matrix::

            gram(hyp)

        takes the training inputs provided during the GP initialization and computes
        G_ij = k(x_i, y_i). The hyperparameters hyp are needed for the 'ap'
        kernel.
        '''

        self.G_ = self.kernel(self.X_, self.X_, hyp)


    # covariance matrix between two vector arrays
    def kernel(self, x, y, hyp):
        '''
        computes the covariance between two vector arrays::

            kxy = kernel(x, y, hyp)

        takes the n x D array x and m x D array y as input and computes the n x m matrix
        kxy that contains the scalar products between all pairs of vectors in x and y. The
        scalar product is chosen according to the covariance type set during
        initialization. The vectors in x and y must have the same dimensionality. In
        addition a vector of hyperparameters must be provided which is needed for
        evaluating the kernels 'g' and 'ap'.
        '''
        # check dimensionality of vectors
        if ((x.ndim > 1 and y.ndim > 1 and np.shape(x)[1] != np.shape(y)[1]) or
            x.ndim != y.ndim):
            self.logger.warn('preg: tried to multiply vector arrays of different dimension.')
            return []


        # inner product
        if x.ndim == 1 and y.ndim == 1:
            self.xy_ = np.outer(x, y)
        else:
            self.xy_ = np.dot(x, y.T)

        if self.covtype == 'ihp': # inhomogeneous polynomial kernel
            kxy = self.xy_ + np.ones(np.shape(self.xy_))
            if self.degree > 1: # nonlinear kernel
                kxy = kxy ** self.degree
        elif self.covtype == 'sp': # summed polynomial kernel
            kxy = self.xy_ + np.ones(np.shape(self.xy_))
            for i in np.arange(2, self.degree+1):
                kxy = kxy + self.xy_ ** i
        elif self.covtype == 'ap': # adaptive polynomial kernel
            kxy = exp(hyp[2])*self.xy_ + np.ones(np.shape(self.xy_))
            for i in np.arange(2, self.degree+1):
                kxy = kxy + exp(hyp[i+1])*(self.xy_ ** i)

        return kxy


    # covariance matrix
    def cov(self, hyp):
        '''
        computes the covariance matrix::

            cov(hyp)

        takes the Gram matrix G of the GP (must be precomputed) and computes the
        covariance matrix according to

            K = vs*G + vn*1

        where 1 is identity matrix, vs the signal variance, and vn the noise variance.
        vs and vn are computed from the first two hyperparameters. If the resulting
        covariance matrix is grossly ill-conditioned, a small ridge is added to improve
        the condition number.
        '''
        vs = hyp[0]; # signal variance
        vn = hyp[1]; # noise variance

        # compute Gram matrix in case it's missing
        if len(self.G_) == 0:
            self.gram(hyp)

        # compute covariance by adding noise delta
        self.K_ = exp(2*vs)*self.G_ + exp(2*vn)*np.eye(np.shape(self.G_)[0])

        # add small ridge if condition number is too large
        if np.linalg.cond(self.K_, np.inf) > 1e8:
            self.K_ = self.K_ + ((1e-5/np.shape(self.K_)[0])*np.trace(np.abs(self.K_))*
                np.eye(np.shape(self.K_)[0]))


    # invert covariance matrix
    def invert_cov(self, modsel):
        '''
        inverts the covariance matrix::

            invert_cov(modsel)

        usually computes the Cholesky inverse of the (precomputed) covariance of the GP.
        If this fails, the pseudo inverse is used instead. In case the model selection
        method is set to 'llh', the method additionally computes the log of the
        determinant which is needed in the log likelihood criterion.
        '''
        # invert covariance
        try: # invert covariance using Cholesky decomposition
            (L, lower) = LA.cho_factor(self.K_)
            self.invK_ = LA.cho_solve((L, lower), np.eye(np.shape(L)[0]))
            if modsel == 'llh':
                self.logdetK_ = (2.0*np.log(np.diag(L))).sum()

        except: # if not positive definite, use pseudo inverse
            if modsel == 'llh':
                U, s, V = np.linalg.svd(self.K_, full_matrices=True)
                S = np.diag(1.0/s)
                self.invK_ = np.dot(V.T, np.dot(S, U.T))
                self.logdetK_ = np.log(s).sum()
            else:
                self.invK_ = LA.pinv(self.K_)


    # objective function
    def objective(self, y, hyp, modsel):
        '''
        evaluate objective function for model selection::

            (f, df, err) = objective(y, hyp, modsel)

        returns the value f of the objective function for given set 'hyp' of
        hyperparameters, and additionally the set of derivatives df of the objective
        function with respect to the hyperparameters. The objective function is either the
        log likelihood (llh), Geissers surrogate predictive probability (gpp), or the
        leave-one-out error (loo), depending on the setting during initialization. The
        analytically computed derivatives df are returned as a vector with the same
        dimensionality as hyp. When 'loo' is chosen as evaluation criterion, the
        derivative w.r.t. the signal standard deviation is always set to 0 (although it is
        not 0 in reality) such that it remains constant during minimization. If anything
        goes wrong the error parameter err is set to 1 (currently unused). The method
        additionally needs the training outputs 'y' and the model selection method.
        '''
        vs = hyp[0]     # signal variance
        vn = hyp[1]         # noise variance
        err = 0                 # set error flag to 0 by default
        df = np.zeros(np.shape(hyp))    # set derivative of f to 0 by default
        d_in = np.zeros(np.shape(hyp)) # inner derivatives

        # sanity check for hyperparameters, otherwise indicate divergence
        if hyp.max() > 50:
            f = 100000000000000     # ^50 makes no sense when
            err = 1                 # input is sensibly normalized.
            return (f, df, err)     # set error flag => divergence
        if np.isnan(hyp).sum() != 0:
            self.logger.warn('preg:  hyperparameters reached NaN!')
            f = 100000000000000     # ^50 makes no sense when
            err = 1                 # input is sensibly normalized.
            return (f, df, err)     # set error flag => divergence

        # center training data
        y = y - self.mu_

        # compute covariance
        if self.covtype == 'ap':
            self.gram(hyp)
        self.cov(hyp)
        dK = np.zeros((np.shape(self.K_)[0], np.shape(self.K_)[1], np.shape(hyp)[0]))

        # dK/dvs
        dK[:,:,0] = self.G_

        # dC/dvn
        dK[:,:,1] = np.eye(np.shape(self.K_)[0])

        # dC/dzeta
        if self.covtype == 'ap':
            dK[:,:,2] = exp(2*vs)*self.xy_
            for i in np.arange(3,np.shape(hyp)[0]):
                dK[:,:,i] = exp(2*vs)*(self.xy_ ** (i-1))

        # inner derivatives
        d_in[0] = 2.0*exp(2.0*vs)
        d_in[1] = 2.0*exp(2.0*vn)
        if self.covtype == 'ap':
            for i in np.arange(2,np.shape(hyp)[0]):
                d_in[i] = exp(hyp[i])

        # invert covariance
        self.invert_cov(modsel)
        self.invKt_ = np.dot(self.invK_, y)

        # log likelihood
        if modsel == 'llh':

            f = 0.5*self.logdetK_ + 0.5*np.dot(y.T, self.invKt_) + (0.5*
                np.shape(self.K_)[0]*log(2.0*pi))

            # ... and its partial derivatives
            for i in np.arange(0, np.shape(hyp)[0]):
                H1 = np.dot(self.invK_, dK[:,:,i])
                H2 = np.dot(self.invKt_.T, np.dot(dK[:,:,i], self.invKt_))
                df[i] = 0.5*d_in[i]*(np.trace(H1) - H2)

        # Geissers surrogate predictive probability
        elif modsel == 'gpp':

            diaginvK = np.diag(self.invK_)
            gspp = 0.5*(np.mean(np.log(diaginvK) - self.invKt_**2/diaginvK) - log(2*pi))
            f = -gspp

            # ... and its negative partial derivatives
            invKtodiaginvK = self.invKt_/diaginvK
            h = 0.5*(1.0/diaginvK + invKtodiaginvK**2)
            for i in np.arange(0, np.shape(hyp)[0]):
                H = np.dot(self.invK_, np.dot(dK[:,:,i], self.invK_))
                df[i] = -(d_in[i]/np.shape(self.K_)[0]) * (np.dot(invKtodiaginvK,
                    np.dot(H, y)) - np.dot(h, np.diag(H)))

        # Leave-one-out error
        elif modsel == 'loo':

            diaginvK = np.diag(self.invK_)
            f = ((self.invKt_/diaginvK)**2.0).mean()

            # ... and its partial derivatives
            h = (self.invKt_**2.0)/(diaginvK**3.0)
            invKtodiaginvK = self.invKt_/(diaginvK**2.0)
            df[0] = 0; # vs is kept constant (loo is invariant w.r.t. scaling)
            for i in np.arange(1, np.shape(hyp)[0]):
                H = np.dot(self.invK_, np.dot(dK[:,:,i], self.invK_))
                df[i] = -(2.0*d_in[i]/np.shape(self.K_)[0]) * (np.dot(invKtodiaginvK,
                    np.dot(H, y)) - np.dot(h, np.diag(H)))

        return (f, df, err)


    # copy the trained model from another instance and re-use it here
    def copy(self, other):
        '''
        Copy the trained model from another Preg instance and re-use it
        here.
        '''

        # copy the optimal parameters from the other instance
        self.covtype = other.covtype
        self.degree = other.degree
        if type(other.hyp) != np.ndarray:
            other.hyp = np.asarray(other.hyp)
        self.hyp = other.hyp.copy()
        self.logger = other.logger

        # internal variables
        self.mu_ = other.mu_                  # training output mean
        self.X_ = other.X_                    # training inputs
        self.G_ = other.G_                    # Gram matrix
        self.K_ = other.K_                    # Covariance matrix
        self.D_ = other.D_                    # Distance matrix (only needed for Gaussian kernel)
        self.xy_ = other.xy_                  # inner product matrix
        self.invK_ = other.invK_              # Inverse covariance matrix
        self.invKt_ = other.invKt_            # invK * target
        self.logdetK_ = other.logdetK_        # log of determinant

    def save(self, path):
        np.savez(path, covtype=self.covtype,
                    degree=self.degree,
                    hyp =self.hyp,
                    mu_=self.mu_,
                    X_=self.X_,
                    G_=self.G_,
                    K_=self.K_,
                    D_=self.D_,
                    xy_=self.xy_,
                    invK_=self.invK_,
                    invKt_=self.invKt_,
                    logdetK_=self.logdetK_)

    def load(self, path):
        gp = np.load(path)
        print(gp.files)
        self.covtype = gp['covtype']
        self.degree = gp['degree']
        self.hyp = gp['hyp']
        self.mu_ = gp['mu_']
        self.X_ = gp['X_']
        self.G_ = gp['G_']
        self.K_ = gp['K_']
        self.D_ = gp['D_']
        self.xy_ = gp['xy_']
        self.invK_ = gp['invK_']
        self.invKt_ = gp['invKt_']
        self.logdetK_= gp['logdetK_']


    # predict mean and variance
    def pred_meanvar(self, X):
        '''
        Computation of the predictive distribution (i.e., predictive mean and variance) of
        the output for a new, previously unseen input::

            means, variances = pred_meanvar(test)

        returns the predicted means and variances for the test input as a numpy vector.
        The test input must be a m x D vector array where the dimensionality of the
        vectors is the same as that of the training data vectors. An automatic
        model selection or fit() has to be run before to get the optimal hyperparameters.
        In addition, most of the computationally expensive terms are precomputed during ams()
        or amsd() which greatly accelerates the prediction.

        Note that predicted variances should be only computed when hyperparameters were
        chosen by either 'llh' or 'gpp'. Optimizing "loo" determines variances only up to
        a scale factor!
        '''
        # self check
        if not self.check_params():
            return []
        if len(self.K_) == 0:
            self.logger.warn('preg: no training data saved, cannot predict.')
            return []

        # check test input
        if type(X) != np.ndarray:
            self.logger.warn('preg: test input is not a *NumPy* array')
            return []
        if X.ndim > 2:
            self.logger.warn('preg: test input must be 1- or 2-dimensional')
            return []
        if ((X.ndim > 1 and self.X_.ndim > 1 and np.shape(X)[1] !=
            np.shape(self.X_)[1]) or X.ndim != self.X_.ndim):
            self.logger.warn('preg: dimension of training and test input is different.')
            return []
        if X.dtype != 'float64':
            X = X.astype('float64')

        # cross-covariance
        vs = self.hyp[0]        # signal variance
        Qt = exp(2*vs)*self.kernel(self.X_, X, self.hyp)

        # predicted means
        pr_mean = np.dot(Qt.T, self.invKt_) + self.mu_

        # predicted variances
        tt = exp(2*vs)*np.diag(self.kernel(X, X, self.hyp))
        pr_var = tt - np.sum(Qt * np.dot(self.invK_, Qt), axis=0)

        return pr_mean, pr_var


    # do full regression: ams, prediction and output error
    def preg(self, X0, y0, X, y, degs, modsel='gpp', nit=10):
        '''
        Convenience function combining amsd(), pred_mean() and error reporting::

            (tr_err, te_err) = preg(nit, test, test_target, degs)

        first calls amsd() for the array of polynomial degrees 'degs' in case the GP is of
        polynomial type, otherwise ams() is called for model selection. Next, predictive
        means are computed for the training and test data 'test' using predict(). From
        the provided training and test targets (parameter 'test_target') the mean square
        error is computed and returned. 'tr_err' is the MSE on the training set, 'te_err'
        the MSE on the test set. 'nit' is the maximal number of line searches during
        minimization. Clearly, the number of test targets must be the same as that of the
        test input vectors.
        '''
        # self check
        if not self.check_params():
            return []

        # check training data
        if not self.check_data(X0, y0):
            return []
        if X0.dtype != 'float64':
            X0 = X0.astype('float64')
        if y0.dtype != 'float64':
            y0 = y0.astype('float64')
        self.mu_ = y0.mean()
        self.X_ = X0.copy()

        # check test data
        if not self.check_data(X, y):
            return []
        if X.dtype != 'float64':
            X = X.astype('float64')
        if y.dtype != 'float64':
            y = y.astype('float64')
        if ((X.ndim > 1 and self.X_.ndim > 1 and np.shape(X)[1] !=
            np.shape(self.X_)[1]) or X.ndim != self.X_.ndim):
            self.logger.warn('preg: dimension of training and test input is different.')
            return []

        # check degrees
        if type(degs) != list:
            degs = list(degs)
        for deg in degs:
            if type(deg) != float:
                deg = float(deg)
            if deg < 1.0:
                self.logger.warn('preg: degree of polynomial kernel must be > 0.')
                return []
            if deg != floor(deg):
                self.logger.warn('preg: degree of polynomial kernel must an integer number.')
                return []

        # check other parameters
        if modsel not in ['llh', 'gpp', 'loo']:
            self.logger.warn('preg: unkown model selection method: {}'.format(modsel))
            return []
        if type(nit) != int:
            nit = int(nit)
        if nit < 1:
            self.logger.warn('preg: number of iterations must be > 0.')
            return []

        # train and test
        self.amsd(self.X_, y0, degs, modsel, nit)
        self.logger.info(
            'poly_type: {:s}, degree: {:d}, model selection: {:s}'.format(self.covtype,
            self.degree, modsel))
        mu_i = self.predict(self.X_)   # prediction on training data
        mu_t = self.predict(X)         # prediction on test data

        # training and test error
        err = (mu_i - y0)**2
        tr_err = err.mean()                 # MSE on training data
        err = (mu_t - y)**2
        te_err = err.mean()                 # MSE on test data
        self.logger.info('   train: mse = {:g}, test: mse = {:g}'.format(tr_err, te_err))

        return tr_err, te_err


    def volt(self, deg):
        '''
        Computation of the explicit nth-order Volterra operator from the implicitly given
        polynomial expansion:

            eta = gp.volt(p)

        returns the explicit  pth-order Volterra operator coefficients in the form of a tensor,
        i.e. if the input is d-dimensional, the tensor of order p is of size d x d x .... p
        times ... x d. The coefficient with index i1 i2 ..... id is the weight factor of the
        monomial x_i1 * x_i2 * ... * x_id in the discrete Volterra operator.
        '''
        # self check
        if not self.check_params():
            return []
        if len(self.K_) == 0:
            self.logger.warn('preg: no training data saved, cannot compute Volterra operator.')
            return []

        # check degree
        if type(deg) != int:
            deg = int(deg)
        if deg < 0:
            self.logger.warn('preg: Volterra operator degree must be >= 0.')
            return []
        if deg > self.degree:
            return 0
        deg = float(deg) # avoid integer division

        # regression weights
        vs = self.hyp[0]        # signal variance
        w = exp(2*vs)*self.invKt_

        # correction factor for different kernel types
        if self.covtype == 'ihp': # inhomogeneous polynomial kernel
            a = factorial(self.degree)/factorial(deg)/factorial(self.degree - deg)
        elif self.covtype == 'sp': # summed polynomial kernel
            a = 1
        elif self.covtype == 'ap': # adaptive polynomial kernel
            a = exp(self.hyp[deg + 2])

        n = np.shape(self.X_)[0] # number of training examples
        d = np.shape(self.X_)[1] # dimensionality of data

        # 0-order operator
        if deg == 0:
            phi = np.ones((1, n), dtype=np.float64)
            return a * np.dot(phi, w)

        # 1st-order operator
        elif deg==1:
            phi = self.X_.T
            return a * np.dot(phi, w)

        # higher-order operator
        else:
            dims = list(d*np.ones(int(deg), dtype=np.int32))
            phi = np.zeros(dims, dtype=np.float64)
            for i in np.arange(0,n):
                datapt = self.X_[i,:]

                # second-order operator
                p = np.outer(datapt, datapt)

                # higher orders
                for j in np.arange(3, deg+1):
                    p = np.tensordot(datapt, p, axes=0)

                # add up with weight
                phi += w[i]*p

            return a*phi


    # Carl's minimize
    def minimize(self, tgt, X, modsel, maxnumlinesearch):
        '''
        minimize an objective function using gradient descent::

            (hyp_opt, ni, f0) = minimize(tgt, hyp, modsel, n_iter)

        minimizes one of the objective functions 'llh', 'gpp', or 'loo' set in the GP
        initialization in hyperparameter space. The starting value for the hyperparameters
        is hyp0, the maximum number of line searches is n_iter. The method returns the
        optimal hyperparameter set 'hyp_opt', the number of iterations 'ni', and the value
        'f0' of the objective function at the found minimum. The training targets 'tgt'
        must be provided for computing the objective function.

        This code is based on Carl Rasmussen's minimize.m MATLAB script (see copyright
        notice in the code) and its Python adaptation by R. Memisevic.
        '''
        #The following is the original copyright notice that comes with the
        #function minimize.m
        #(from http://www.kyb.tuebingen.mpg.de/bs/people/carl/code/minimize/Copyright):
        #
        #"(C) Copyright 1999 - 2006, Carl Edward Rasmussen
        #
        #Permission is granted for anyone to copy, use, or modify these
        #programs and accompanying documents for purposes of research or
        #education, provided this copyright notice is retained, and note is
        #made of any changes that have been made.
        #
        #These programs and documents are distributed without any warranty,
        #express or implied.  As the programs were written for research
        #purposes only, they have not been tested to the degree that would be
        #advisable in any important application.  All use of these programs is
        #entirely at the user's own risk."
        #
        #   Changes to the original version:
        #   - direct call to gp.objective()
        #   - disabled argument maxnumfunevals, red
        #
        #   Changes to R. Memisevic's version:
        #   - corrected sign of length = -maxnumfuneval
        #   - numpy calls with np prefix

        INT = 0.1;# don't reevaluate within 0.1 of the limit of the current bracket
        EXT = 3.0;              # extrapolate maximum 3 times the current step-size
        MAX = 20;                     # max 20 function evaluations per line search
        RATIO = 10;                                   # maximum allowed slope ratio
        SIG = 0.1;RHO = SIG/2;# SIG and RHO are the constants controlling the Wolfe-
        #Powell conditions. SIG is the maximum allowed absolute ratio between
        #previous and new slopes (derivatives in the search direction), thus setting
        #SIG to low (positive) values forces higher precision in the line-searches.
        #RHO is the minimum allowed fraction of the expected (from the slope at the
        #initial point in the linesearch). Constants must satisfy 0 < RHO < SIG < 1.
        #Tuning of SIG (depending on the nature of the function to be optimized) may
        #speed up the minimization; it is probably not worth playing much with RHO.

        SMALL = 10.**-16                    #minimize.m uses matlab's realmin
        maxnumfuneval = None
        red = 1.0
        S = 'Linesearch'
        length = maxnumlinesearch

        i = 0                                         # zero the run length counter
        ls_failed = 0                          # no previous line search has failed
        (f0, df0, e) = self.objective(tgt, X, modsel)          # get function value and gradient
        i = i + (length<0)                                         # count epochs?!
        s = -df0
        d0 = -np.dot(s,s)    # initial search direction (steepest) and slope
        x3 = red/(1.0-d0)                             # initial step is red/(|s|+1)

        while i < abs(length):                                 # while not finished
            i = i + (length>0)                                 # count iterations?!

            X0 = X; F0 = f0; dF0 = df0              # make a copy of current values
            if length>0:
                M = MAX
            else:
                M = min(MAX, -length-i)
            while 1:                      # keep extrapolating as long as necessary
                x2 = 0; f2 = f0; d2 = d0; f3 = f0; df3 = df0
                success = 0
                while (not success) and (M > 0):
                    try:
                        M = M - 1; i = i + (length<0)              # count epochs?!
                        (f3, df3, e) = self.objective(tgt, X+x3*s, modsel)
                        if (np.isnan(f3) or np.isinf(f3) or
                            np.any(np.isnan(df3)+np.isinf(df3))):
                            self.logger.warn("preg: error in minimize.")
                            return
                        success = 1
                    except:                    # catch any error which occured in f
                        x3 = (x2+x3)/2                       # bisect and try again
                if f3 < F0:
                    X0 = X+x3*s; F0 = f3; dF0 = df3   # keep best values
                d3 = np.dot(df3,s)                                         # new slope
                if d3 > SIG*d0 or f3 > f0+x3*RHO*d0 or M == 0:
                                                       # are we done extrapolating?
                    break
                x1 = x2; f1 = f2; d1 = d2                 # move point 2 to point 1
                x2 = x3; f2 = f3; d2 = d3                 # move point 3 to point 2
                A = 6*(f1-f2)+3*(d2+d1)*(x2-x1)          # make cubic extrapolation
                B = 3*(f2-f1)-(2*d1+d2)*(x2-x1)
                Z = B + np.sqrt(complex(B*B-A*d1*(x2-x1)))
                if Z != 0.0:
                    x3 = x1-d1*(x2-x1)**2/Z              # num. error possible, ok!
                else:
                    x3 = np.inf
                if (not np.isreal(x3)) or np.isnan(x3) or np.isinf(x3) or (x3 < 0):
                                                           # num prob | wrong sign?
                    x3 = x2*EXT                        # extrapolate maximum amount
                elif x3 > x2*EXT:           # new point beyond extrapolation limit?
                    x3 = x2*EXT                        # extrapolate maximum amount
                elif x3 < x2+INT*(x2-x1):  # new point too close to previous point?
                    x3 = x2+INT*(x2-x1)
                x3 = np.real(x3)

            while (abs(d3) > -SIG*d0 or f3 > f0+x3*RHO*d0) and M > 0:
                                                               # keep interpolating
                if (d3 > 0) or (f3 > f0+x3*RHO*d0):            # choose subinterval
                    x4 = x3; f4 = f3; d4 = d3             # move point 3 to point 4
                else:
                    x2 = x3; f2 = f3; d2 = d3             # move point 3 to point 2
                if f4 > f0:
                    x3 = x2-(0.5*d2*(x4-x2)**2)/(f4-f2-d2*(x4-x2))
                                                          # quadratic interpolation
                else:
                    A = 6*(f2-f4)/(x4-x2)+3*(d4+d2)           # cubic interpolation
                    B = 3*(f4-f2)-(2*d2+d4)*(x4-x2)
                    if A != 0:
                        x3=x2+(np.sqrt(B*B-A*d2*(x4-x2)**2)-B)/A
                                                         # num. error possible, ok!
                    else:
                        x3 = np.inf
                if np.isnan(x3) or np.isinf(x3):
                    x3 = (x2+x4)/2      # if we had a numerical problem then bisect
                x3 = max(min(x3, x4-INT*(x4-x2)),x2+INT*(x4-x2))
                                                           # don't accept too close
                (f3, df3, e) = self.objective(tgt, X+x3*s, modsel)
                if f3 < F0:
                    X0 = X+x3*s; F0 = f3; dF0 = df3              # keep best values
                M = M - 1; i = i + (length<0)                      # count epochs?!
                d3 = np.dot(df3,s)                                      # new slope

            if abs(d3) < -SIG*d0 and f3 < f0+x3*RHO*d0:  # if line search succeeded
                X = X+x3*s; f0 = f3;                          # update variables
                self.logger.debug('{:s} {:6d};  Value {:4.6e}\r'.format(S, i, f0))
                s = (np.dot(df3,df3)-np.dot(df0,df3))/np.dot(df0,df0)*s - df3
                                                      # Polack-Ribiere CG direction
                df0 = df3                                        # swap derivatives
                d3 = d0; d0 = np.dot(df0,s)
                if d0 > 0:                             # new slope must be negative
                    s = -df0; d0 = -np.dot(s,s)  # otherwise use steepest direction
                x3 = x3 * min(RATIO, d3/(d0-SMALL))     # slope ratio but max RATIO
                ls_failed = 0                       # this line search did not fail
            else:
                X = X0; f0 = F0; df0 = dF0              # restore best point so far
                if ls_failed or (i>abs(length)):# line search failed twice in a row
                    break                    # or we ran out of time, so we give up
                s = -df0; d0 = -np.dot(s,s)                          # try steepest
                x3 = 1/(1-d0)
                ls_failed = 1                             # this line search failed
        self.logger.debug("Best value: {:4.6e}".format(f0))
        return X, i, f0

