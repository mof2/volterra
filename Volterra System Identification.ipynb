{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b4d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, sqrt, log, exp\n",
    "\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from preg import Preg, Logger\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20/2.54, 16/2.54]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412a089",
   "metadata": {},
   "source": [
    "# 1. Linear system identification\n",
    "\n",
    "## a. Linear system\n",
    "\n",
    "lin_sys() defines the right hand side of a 2D linear system of the form\n",
    "\n",
    "$$\\dot{y} = A y + u $$\n",
    "\n",
    "with a stable spiral fixed point attractor. We start with zero forcing u = 0 and let the resulting autonomous system converge to the fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b21c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero forcing for simulating autonomous system\n",
    "def zero_forcing(t):\n",
    "    return 0.0\n",
    "\n",
    "# Linear system with a stable spiral fixed point attractor.\n",
    "def lin_sys(y,t, inp):\n",
    "    u = inp(t)\n",
    "    return np.dot(np.array([[-1, -2], [3, -1]]), np.array([y[0],y[1]])+\\\n",
    "        np.array([u,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simulation: converge to the fixed point attractor of the autonomous\n",
    "# system\n",
    "n = 100                     # number of time steps\n",
    "t0, t1 = 0, 10              # start and end\n",
    "t = np.linspace(t0, t1, n)  # the points of evaluation of solution\n",
    "h = t[1]-t[0]               # time step\n",
    "y0 = [4, 1]                 # initial value\n",
    "y = integrate.odeint(lin_sys, y0, t, (zero_forcing,))\n",
    "\n",
    "plt.plot(t, y)\n",
    "plt.title('convergence to fixed point attractor of autonomous system')\n",
    "plt.xlabel('time [s]')\n",
    "plt.grid(True)\n",
    "plt.legend(('y_1', 'y_2'),loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# The attractor (we know it's simply the origin (0,0))\n",
    "y_a = y[-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ae0c5",
   "metadata": {},
   "source": [
    "## b. System identification from step response\n",
    "\n",
    "For this, we simulate the system with step forcing. The impulse response is obtained by differentiating the step response. The impulse response is identical to the first-order Volterra kernel of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_forcing(t):\n",
    "    if t >= 0:\n",
    "        return 1.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e49af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Force the system by a step-wise input u(t) being initially situated on\n",
    "# the attractor\n",
    "y_step = integrate.odeint(lin_sys, y_a, t, (step_forcing,))\n",
    "\n",
    "plt.plot(t, y_step)\n",
    "plt.title('step response')\n",
    "plt.xlabel('time [s]')\n",
    "plt.grid(True)\n",
    "plt.legend(('y_1', 'y_2'),loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The linear or first-order (1) impulse response function (IRF), a.k.a.\n",
    "# Volterra kernel: http://www.scholarpedia.org/article/Volterra_and_Wiener_series\n",
    "h_1 = np.diff(y_step[:,0], prepend=0)\n",
    "\n",
    "plt.plot(t, h_1)\n",
    "plt.title('impulse response $y_1$')\n",
    "plt.xlabel('time [s]')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16551e",
   "metadata": {},
   "source": [
    "## c. Test the impulse response on a sine input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_forcing(t):\n",
    "    om = 1\n",
    "    return sin(om*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Test the prediction performed by using the IRF; apply any desirable\n",
    "# forcing\n",
    "y_sine = integrate.odeint(lin_sys, y_a, t, (sine_forcing,))\n",
    "u_sine = np.array([sine_forcing(t2) for t2 in t])\n",
    "y_pr1 = np.convolve(u_sine, h_1, 'full')\n",
    "y_pr1 = y_pr1[:n]\n",
    "\n",
    "plt.plot(t, y_sine[:,0], '-o', t, y_pr1, '-x')\n",
    "plt.title('sine response')\n",
    "plt.xlabel('time [s]')\n",
    "plt.grid(True)\n",
    "plt.legend(('true model output', 'IRF prediction'),loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf73a36",
   "metadata": {},
   "source": [
    "## d. System identification via linear regression\n",
    "\n",
    "Linear regression needs data in the form of input-output pairs $(x_i,y_i)$. $y_i = y(t_i)$ is the system output at time $t_i$, $x_i = (x(t_{i-m}), x(t_{i-m+1}), \\dots, x(t_{i-1}), x(t_i))$ the current input and all past inputs that affect the system output. $m$ is the memory of the system. This data format is obtained by sliding a window of size $m$ over the input time series, the associated output is the element of the output time series that corresponds to the last input of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ea1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert step input to sliding window format for regression (see 4.)\n",
    "memory = n # how may sampling points back in time are used as input for the Volterra operators\n",
    "u1 = np.zeros(n + memory - 1)\n",
    "u1[memory:] = np.ones(n-1) # create step input by prepending zeros\n",
    "x0 = np.zeros((n, memory))\n",
    "for i in range(0,n):\n",
    "    x0[i,:] = u1[i:i + memory]\n",
    "y0 = y_step[:,0] # associated output\n",
    "\n",
    "# convert sine input to sliding window format for regression (see 4.)\n",
    "u2 = np.zeros(n + memory - 1)\n",
    "u2[memory-1:] = u_sine\n",
    "x1 = np.zeros((n, memory))\n",
    "for i in range(0,n):\n",
    "    x1[i,:] = u2[i:i + memory]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66df1c7",
   "metadata": {},
   "source": [
    "We train and test on the same time series as above, converted to the sliding window format. The first-order Volterra operator obtained from *preg* is given as coefficient vector $\\eta = (\\eta_0, \\eta_1, \\dots, \\eta_m)$ such that the system output is computed as\n",
    "$$ y_i = \\sum_{j=0}^m \\eta_{m-j} x_{i-j}, $$\n",
    "so the coefficients from *preg* just keep the same order as the input window. To obtain the classical convolution notation \n",
    "$$ y_i = \\sum_{j=i-m}^i x_j \\eta'_{m-j} , $$\n",
    "the first-order kernel must be flipped, i.e., $\\eta'_j = \\eta_{m-j}$. The flipped kernel is identical to the impulse response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Obtain h_1 using poly_reg\n",
    "with Logger('linear') as lg: # get a logger instance\n",
    "\n",
    "    ptype = 'ihp'\t\t# kernel type ('ihp' or 'ap')\n",
    "    method = 'gpp'\t\t# model selection method ('llh', 'gpp', 'loo')\n",
    "    n_iter = 20\t\t    # number of iterations\n",
    "    order = 1           # Volterra series order\n",
    "\n",
    "    # regression\n",
    "    hp0 = [log(0.6), log(sqrt(0.001))]\n",
    "    gp = Preg(lg, ptype, order, hp0) # init GP struct\n",
    "    gp.ams(x0, y0, method, n_iter)     # do regression\n",
    "    mu_step = gp.predict(x0)    # predict on training input\n",
    "\n",
    "    # plot prediction on training set (step function)\n",
    "    plt.plot(t, mu_step, '-o', t, y0, '-x')\n",
    "    plt.title('Linear prediction on training set')\n",
    "    plt.xlabel('time [s]')\n",
    "    plt.legend(('preg prediction','true model output'),loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # prediction on test set (sine)\n",
    "    mu_sine = gp.predict(x1)\n",
    "    plt.plot(t, mu_sine, '-o', t, y_sine[:,0], '-x')\n",
    "    plt.title('Linear prediction on test set')\n",
    "    plt.xlabel('time [s]')\n",
    "    plt.legend(('preg prediction','true model output'),loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # compute explicit 1st-order Volterra operator\n",
    "    eta = gp.volt(order)\n",
    "    eta = np.flip(eta) # flip to obtain impulse response\n",
    "    plt.plot(t[:n-1], eta[:n-1], '-o', t[:n-1], np.diff(y_step[:,0]), '-x')\n",
    "    plt.title('First order Volterra kernel')\n",
    "    plt.xlabel('Delay [s]')\n",
    "    plt.legend(('preg','impulse response'),loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a5a37",
   "metadata": {},
   "source": [
    "# Nonlinear system identification\n",
    "\n",
    "## Second-order nonlinear system\n",
    "\n",
    "Oor second-order nonlinear system example is simply the linear system from above where the output is fed through a $x^2()$ nonlinearity:\n",
    "$$\\dot{y} = (A y + u)^2. $$\n",
    "Since we want to test our regression on a smooth sine input, it is useful to obtain our training data from a time series with a similar degree of smoothness. Here, we just concatenate flipped and unflipped versions of the previous step response as our training input time series and feed it through the nonlinear system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 2nd-order nonlinear system with smooth input\n",
    "xtr = np.concatenate((y_step[:,1], np.flip(y_step[:,1]), y_step[:,1],))\n",
    "ntr = len(xtr)\n",
    "y_nl = np.convolve(xtr, h_1, 'full')\n",
    "y_nl = y_nl[:ntr+1]\n",
    "y_nl = y_nl**2\n",
    "ttr = np.arange(0,h*(ntr - memory + 1),h)\n",
    "x2 = np.zeros((len(ttr), memory))\n",
    "y2 = np.zeros((len(ttr),))\n",
    "for i in range(0,len(y2)):\n",
    "    x2[i,:] = xtr[i:i + memory]\n",
    "    y2[i] = y_nl[i + memory - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f40a6",
   "metadata": {},
   "source": [
    "Again, *preg* gives the second-order Volterra kernel in the form\n",
    "$$ y_i = \\sum_{j=0}^m \\sum_{k=0}^m \\eta_{m-j,m-k} x_{i-j}x_{i-k} $$. \n",
    "For the tradiational notation, the resulting 2D kernel needs to be flipped in both temporal directions $\\eta'_{jk} = \\eta_{m-j,m-k}$ such that\n",
    "$$ y_i = \\sum_{j=i-m}^i \\sum_{k=i-m}^i \\eta_{m-j,m-k} x_j x_k $$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b80171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Obtain second-order Volterra kernel for a nonlinear operator\n",
    "# do regression and predict on training data\n",
    "with Logger('nonlinear') as lg: # get a logger instance\n",
    "\n",
    "    order = 2           # Volterra series order\n",
    "\n",
    "    # regression\n",
    "    hp0 = [log(0.6), log(sqrt(0.001))]\n",
    "    gp_nl = Preg(lg, ptype, order, hp0) # init GP struct\n",
    "    gp_nl.ams(x2, y2, method, n_iter)     # do regression\n",
    "    mu_nl = gp_nl.predict(x2)    # predict on training input\n",
    "\n",
    "    # plot prediction on training set\n",
    "    plt.plot(ttr, mu_nl, ttr, y2) #, ttr, xtr[memory-1:memory+len(ttr)-1])\n",
    "    plt.title('Nonlinear prediction on training set')\n",
    "    plt.xlabel('time [s]')\n",
    "    plt.legend(('prediction','true model output','input'),loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # prediction on test set (sine)\n",
    "    mu_sine_nl = gp_nl.predict(x1)\n",
    "    plt.plot(t, mu_sine_nl, '-o', t, y_sine[:,0]**2, '-x')\n",
    "    plt.title('Nonlinear prediction on test set')\n",
    "    plt.xlabel('time [s]')\n",
    "    plt.legend(('prediction','true model output'),loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # 2nd-order Volterra kernel\n",
    "    eta = gp_nl.volt(order)\n",
    "    eta = np.flipud(np.fliplr(eta)) # time delay is measured in reverse time direction\n",
    "                                    # kernel mst be flipped for standrad display\n",
    "                                    # convention\n",
    "    xv, yv = np.meshgrid(t, t)\n",
    "    plt.rcParams['figure.figsize'] = [30/2.54, 24/2.54]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(xv, yv, eta, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    ax.set_xlabel('delay [s]')\n",
    "    ax.set_ylabel('delay [s]')\n",
    "    plt.title('Second order Volterra kernel')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8764b012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
